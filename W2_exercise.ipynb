{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enisba/inzva_DLSG_notebook/blob/main/W2_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bafe7a6",
      "metadata": {
        "id": "3bafe7a6"
      },
      "source": [
        "# WEEK 2: Deep Learning Fundamentals with Fashion MNIST\n",
        "\n",
        "---\n",
        "\n",
        "## LEARNING OBJECTIVES\n",
        "By the end of this exercise, you will be able to:\n",
        "1. Load and preprocess image data using PyTorch's DataLoader\n",
        "2. Design a feedforward neural network architecture\n",
        "3. Implement a complete training loop with forward/backward propagation\n",
        "4. Apply regularization techniques (L1, L2, Dropout) to prevent overfitting\n",
        "5. Use Early Stopping to optimize training duration\n",
        "6. Perform hyperparameter tuning using Grid Search and Random Search\n",
        "\n",
        "## DATASET: Fashion MNIST\n",
        "- 70,000 grayscale images (28Ã—28 pixels) of 10 clothing categories\n",
        "- Categories: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "- Training set: 60,000 images | Test set: 10,000 images\n",
        "\n",
        "## INSTRUCTIONS\n",
        "- Look for `# TODO:` comments - these are YOUR tasks to complete\n",
        "- Each TODO has a **HINT** and **EXPECTED OUTPUT** to guide you\n",
        "- Use the verification cells to check your work before moving on\n",
        "- Ask questions if you're stuck"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d5a1ba",
      "metadata": {
        "id": "c0d5a1ba"
      },
      "source": [
        "---\n",
        "## Section 1: Imports & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06a1c78",
      "metadata": {
        "id": "a06a1c78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e4cedc-262a-4d31-e8be-06f68acf19a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All imports successful!\n",
            " PyTorch version: 2.9.0+cu126\n",
            " CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch                              # Core tensor operations (like NumPy but with GPU support)\n",
        "import torch.nn as nn                     # Neural network building blocks (layers, activations)\n",
        "import torch.optim as optim               # Optimization algorithms (SGD, Adam, etc.)\n",
        "import torchvision                        # Computer vision datasets and transforms\n",
        "import torchvision.transforms as transforms  # Image preprocessing utilities\n",
        "from torch.utils.data import DataLoader, random_split  # Data loading utilities\n",
        "import numpy as np                        # Numerical computing\n",
        "import matplotlib.pyplot as plt           # Visualization\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import accuracy_score  # Model evaluation metrics\n",
        "import random                             # Random number generation\n",
        "from itertools import product             # Can use for grid search combinations\n",
        "\n",
        "# Set random seeds for reproducibility (important for debugging!)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\" All imports successful!\")\n",
        "print(f\" PyTorch version: {torch.__version__}\")\n",
        "print(f\" CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0063c6c2",
      "metadata": {
        "id": "0063c6c2"
      },
      "source": [
        "---\n",
        "## Section 2: Data Loading & Preprocessing\n",
        "\n",
        "### CONCEPT: Data Transforms\n",
        "Before feeding images to a neural network, we must convert them to tensors.\n",
        "\n",
        "`transforms.ToTensor()` does two things:\n",
        "1. Converts PIL Image to Tensor\n",
        "2. Scales pixel values from [0, 255] to [0.0, 1.0]\n",
        "\n",
        " **Resources:**\n",
        "- [ToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html)\n",
        "- [Normalize](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3143a4c0",
      "metadata": {
        "id": "3143a4c0"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "120f9de0",
      "metadata": {
        "id": "120f9de0"
      },
      "source": [
        "### TODO 1: Load the Fashion MNIST datasets\n",
        "\n",
        "**HINT:** Use `torchvision.datasets.FashionMNIST()`\n",
        "- Required parameters: `root='./data'`, `train=True/False`, `download=True`, `transform=transform`\n",
        "\n",
        "**EXPECTED:**\n",
        "- `train_dataset` should have 60,000 samples\n",
        "- `test_dataset` should have 10,000 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d585b63",
      "metadata": {
        "id": "0d585b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4a6f10-0687-4bcb-f53d-f899aad08aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:02<00:00, 12.1MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 205kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:01<00:00, 3.80MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 23.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# TODO 1: Load Fashion MNIST datasets\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc480ada",
      "metadata": {
        "id": "dc480ada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8658cf85-3f7b-4aac-84a3-acff5e8a1950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 60000\n",
            "Test samples: 10000\n",
            "Image shape: torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# VERIFICATION - Run this cell after completing TODO 1\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Image shape: {train_dataset[0][0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c37b5d",
      "metadata": {
        "id": "80c37b5d"
      },
      "source": [
        "### TODO 2: Split the training data into training and validation sets\n",
        "\n",
        "**WHY?** Validation data helps us monitor for overfitting during training. We never train on validation data - it's our \"practice test.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903e0e12",
      "metadata": {
        "id": "903e0e12"
      },
      "outputs": [],
      "source": [
        "# TODO 2: Split training data into train and validation sets\n",
        "\n",
        "train_size = 50000\n",
        "val_size = 10000\n",
        "\n",
        "# Uncomment after filling above:\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b96a7b9",
      "metadata": {
        "id": "0b96a7b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f594e4b7-ec47-4b3e-cdc0-91bba08490df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training split: 50000 samples\n",
            "Validation split: 10000 samples\n"
          ]
        }
      ],
      "source": [
        "# VERIFICATION - Run this cell after completing TODO 2\n",
        "print(f\"Training split: {len(train_dataset)} samples\")\n",
        "print(f\"Validation split: {len(val_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2dc7fb7",
      "metadata": {
        "id": "d2dc7fb7"
      },
      "source": [
        "### TODO 3: Create DataLoaders for efficient batching\n",
        "\n",
        "**WHY DataLoaders?**\n",
        "- **Automatic batching:** Groups samples into mini-batches for training\n",
        "- **Shuffling:** Randomizes order each epoch to prevent learning data order\n",
        "\n",
        "**HINT:** `DataLoader(dataset, batch_size=64, shuffle=True/False)`\n",
        "- Training: `shuffle=True` (prevents memorizing order)\n",
        "- Validation/Test: `shuffle=False` (consistent evaluation)\n",
        "\n",
        "**Resource:** [DataLoader Documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44950897",
      "metadata": {
        "id": "44950897"
      },
      "outputs": [],
      "source": [
        "# TODO 3: Create DataLoaders\n",
        "\n",
        "BATCH_SIZE = 64  # Number of samples per gradient update\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c814548e",
      "metadata": {
        "id": "c814548e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a8c269-525f-47ba-8b9a-de3c4052456f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: torch.Size([64, 1, 28, 28]) (expected: [64, 1, 28, 28])\n",
            "Labels shape: torch.Size([64]) (expected: [64])\n"
          ]
        }
      ],
      "source": [
        "# VERIFICATION - Run this cell after completing TODO 3\n",
        "sample_batch, sample_labels = next(iter(train_loader))\n",
        "print(f\"Batch shape: {sample_batch.shape} (expected: [64, 1, 28, 28])\")\n",
        "print(f\"Labels shape: {sample_labels.shape} (expected: [64])\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c6616c",
      "metadata": {
        "id": "a7c6616c"
      },
      "source": [
        "---\n",
        "## Section 3: Neural Network Architecture\n",
        "\n",
        "### CONCEPT: Feedforward Neural Network (Multi-Layer Perceptron)\n",
        "\n",
        "**Architecture (optional architecture):**\n",
        "```\n",
        "Input (784) â†’ Hidden1 (256) â†’ Hidden2 (128) â†’ Output (10)\n",
        "```\n",
        "\n",
        "- **Input:** 28Ã—28 = 784 pixels (flattened image)\n",
        "- **Hidden layers:** Learn increasingly abstract features\n",
        "- **Output:** 10 neurons (one per clothing category)\n",
        "\n",
        "**Key Components:**\n",
        "- `nn.Linear(in, out)`: Fully connected layer (y = Wx + b)\n",
        "- `nn.ReLU()`: Activation function (introduces non-linearity)\n",
        "- `nn.Dropout(p)`: Randomly zeros p% of neurons (prevents overfitting)\n",
        "\n",
        "**Resources:**\n",
        "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "- [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4b12beac",
      "metadata": {
        "id": "4b12beac"
      },
      "outputs": [],
      "source": [
        "class FashionMNIST_NN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward neural network for Fashion MNIST classification.\n",
        "\n",
        "    Architecture:\n",
        "        Input (784) â†’ FC1 (256) â†’ ReLU â†’ Dropout â†’ FC2 (128) â†’ ReLU â†’ Dropout â†’ FC3 (10) (optional, you can try different architectures)\n",
        "\n",
        "    Args:\n",
        "        dropout_rate (float): Probability of dropping neurons (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FashionMNIST_NN, self).__init__()\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # TODO 4: Define the network layers\n",
        "        # ---------------------------------------------------------------------\n",
        "        # HINT: Create three linear layers and dropout\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: Defines how data flows through the network.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, 10) - raw scores (logits)\n",
        "        \"\"\"\n",
        "        # ---------------------------------------------------------------------\n",
        "        # TODO 5: Implement the forward pass\n",
        "        # ---------------------------------------------------------------------\n",
        "        # STEP 1: Flatten the input from (batch, 1, 28, 28) to (batch, 784)\n",
        "        #         HINT: x = x.view(x.size(0), -1)  OR  x = x.flatten(start_dim=1)\n",
        "        #\n",
        "        # STEP 2: Pass through layers in order:\n",
        "        #         fc1 â†’ relu â†’ dropout â†’ fc2 â†’ relu â†’ dropout â†’ fc3\n",
        "        #\n",
        "        # IMPORTANT: Do NOT apply softmax here! CrossEntropyLoss expects raw logits.\n",
        "        #\n",
        "        # EXPECTED OUTPUT SHAPE: (batch_size, 10)\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        x = x.flatten(start_dim=1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        out = self.fc3(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "df8ae7ce",
      "metadata": {
        "id": "df8ae7ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34bd3730-ee78-453c-9b2d-abca0965103c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output shape: torch.Size([64, 10])\n",
            "Total parameters: 235,146\n"
          ]
        }
      ],
      "source": [
        "# ðŸ” VERIFICATION - Run this cell after completing TODOs 4 & 5\n",
        "test_model = FashionMNIST_NN(dropout_rate=0.5)\n",
        "test_input = torch.randn(64, 1, 28, 28)  # Fake batch\n",
        "test_output = test_model(test_input)\n",
        "print(f\"Model output shape: {test_output.shape}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55788db",
      "metadata": {
        "id": "a55788db"
      },
      "source": [
        "---\n",
        "## Section 4: Weight Initialization\n",
        "\n",
        "### CONCEPT: Why Initialize Weights Carefully?\n",
        "\n",
        "Poor initialization can cause:\n",
        "- **Vanishing gradients:** Signals shrink to zero â†’ no learning\n",
        "- **Exploding gradients:** Signals grow huge â†’ unstable training\n",
        "\n",
        "**Common Methods:**\n",
        "- **Xavier (Glorot):** Best for tanh/sigmoid activations\n",
        "- **Kaiming (He):** Best for ReLU activations â† We use ReLU, so this is preferred!\n",
        "\n",
        "**Resource:** [PyTorch nn.init](https://pytorch.org/docs/stable/nn.init.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "77ea3d6c",
      "metadata": {
        "id": "77ea3d6c"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model, method='kaiming'):\n",
        "    \"\"\"\n",
        "    Initialize the weights of a neural network.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        method: 'xavier' or 'kaiming' (default: 'kaiming' for ReLU networks)\n",
        "    \"\"\"\n",
        "    for layer in model.children():\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            if method == 'xavier':\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "            elif method == 'kaiming':\n",
        "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "            # Initialize biases to zero\n",
        "            if layer.bias is not None:\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    print(f\"Weights initialized using {method} method\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9815cc1e",
      "metadata": {
        "id": "9815cc1e"
      },
      "source": [
        "---\n",
        "## Section 5: Early Stopping (Provided - Study This Code)\n",
        "\n",
        "### CONCEPT: Early Stopping\n",
        "\n",
        "**Problem:** Training too long leads to overfitting.\n",
        "\n",
        "**Solution:** Monitor validation loss and stop when it stops improving.\n",
        "\n",
        "**How it works:**\n",
        "1. Track the best validation loss seen so far\n",
        "2. If validation loss doesn't improve for `patience` epochs, stop training\n",
        "3. Save the best model weights to restore later\n",
        "\n",
        "This is a form of regularization - it prevents the model from overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6a3bdc47",
      "metadata": {
        "id": "6a3bdc47"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Stops training when validation loss stops improving.\n",
        "\n",
        "    Args:\n",
        "        patience (int): How many epochs to wait for improvement\n",
        "        delta (float): Minimum change to qualify as an improvement\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss  # We want to maximize score (minimize loss)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            # First epoch - save as best\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            # No improvement\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            # Improvement! Reset counter\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
        "        self.best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
        "        print(f\"Checkpoint saved! (val_loss: {val_loss:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2137f48",
      "metadata": {
        "id": "b2137f48"
      },
      "source": [
        "---\n",
        "## Section 6: Training Function\n",
        "\n",
        "### CONCEPT: The Training Loop\n",
        "\n",
        "Every epoch consists of:\n",
        "\n",
        "**1. TRAINING PHASE** (`model.train()`):\n",
        "   - Forward pass: Compute predictions\n",
        "   - Loss calculation: Measure how wrong we are\n",
        "   - Backward pass: Compute gradients\n",
        "   - Optimization: Update weights\n",
        "\n",
        "**2. VALIDATION PHASE** (`model.eval()`):\n",
        "   - Forward pass only (no gradient computation)\n",
        "   - Measure validation loss to monitor overfitting\n",
        "\n",
        "### CONCEPT: Regularization\n",
        "- **L1 (Lasso):** Encourages sparse weights (many zeros) â†’ Feature selection\n",
        "- **L2 (Ridge):** Encourages small weights â†’ Prevents any single feature from dominating\n",
        "- **Dropout:** Randomly \"turns off\" neurons â†’ Forces redundancy in learned features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49e298a",
      "metadata": {
        "id": "c49e298a"
      },
      "outputs": [],
      "source": [
        "def train_model(l1_lambda=0, l2_lambda=0.01, dropout_rate=0.5, weight_init='kaiming', lr=0.01):\n",
        "    \"\"\"\n",
        "    Train a Fashion MNIST classifier with regularization.\n",
        "\n",
        "    Args:\n",
        "        l1_lambda: L1 regularization strength (0 = no L1)\n",
        "        l2_lambda: L2 regularization strength (0 = no L2)\n",
        "        dropout_rate: Dropout probability (0 = no dropout)\n",
        "        weight_init: Weight initialization method ('xavier' or 'kaiming')\n",
        "        lr: Learning rate for optimizer\n",
        "\n",
        "    Returns:\n",
        "        model: Trained model\n",
        "        train_losses: List of training losses per epoch\n",
        "        val_losses: List of validation losses per epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 1: Create and initialize the model\n",
        "    # =========================================================================\n",
        "    # TODO 6: Create the model with the specified dropout rate\n",
        "    # =========================================================================\n",
        "\n",
        "    model = None\n",
        "    # initialize_weights(model, weight_init)  # Uncomment after creating model\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 2: Define the optimizer\n",
        "    # =========================================================================\n",
        "    # TODO 7: Create an SGD optimizer with L2 regularization\n",
        "    #\n",
        "    # WHY SGD? It's the foundational optimizer - understanding it helps you\n",
        "    # appreciate more advanced optimizers like Adam.\n",
        "    #\n",
        "    # Check weight decay parameter of SGD for L2 regularization. Set weight_decay=l2_lambda, if u want to use L2 regularization\n",
        "    #\n",
        "    # HINT:https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
        "    # =========================================================================\n",
        "\n",
        "    optimizer = None\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 3: Define the loss function\n",
        "    # =========================================================================\n",
        "    # TODO 8: Create a CrossEntropyLoss criterion\n",
        "    #\n",
        "    # WHY CrossEntropyLoss for classification?\n",
        "    # - It combines LogSoftmax + NLLLoss in one optimized operation. If you want to use Softmax explicitly, you should use nn.NLLLoss after applying nn.LogSoftmax.\n",
        "    # However, you do not need to do this here since we are using CrossEntropyLoss. Read the documentation below.\n",
        "    # - Perfect for multi-class classification (choosing 1 of N classes)\n",
        "    # - Input: Raw logits (NOT softmax probabilities!)\n",
        "    # - Target: Class indices (0-9 for Fashion MNIST)\n",
        "    #\n",
        "    # HINT: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "    # =========================================================================\n",
        "\n",
        "    criterion = None\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(patience=20)\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 4: Training loop\n",
        "    # =========================================================================\n",
        "    num_epochs = 100  # Maximum epochs (early stopping may end sooner)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRAINING STARTED\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # =====================================================================\n",
        "        # TRAINING PHASE\n",
        "        # =====================================================================\n",
        "        model.train()  # Enable dropout and batch norm training behavior\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            # -----------------------------------------------------------------\n",
        "            # TODO 9: Implement the training step\n",
        "            # -----------------------------------------------------------------\n",
        "            # STEP A: Forward pass - get model predictions\n",
        "            #\n",
        "            # STEP B: Compute loss\n",
        "            #\n",
        "            # STEP C: Add L1 regularization (if l1_lambda > 0) (optional)\n",
        "            #         L1 norm = sum of absolute values of all parameters\n",
        "            #         l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            #         loss = loss + l1_lambda * l1_norm\n",
        "            #\n",
        "            # STEP D: Zero gradients (WHY? Gradients accumulate by default!)\n",
        "            # STEP E: Backward pass - compute gradients\n",
        "            # STEP F: Update weights\n",
        "            # -----------------------------------------------------------------\n",
        "\n",
        "            pass\n",
        "\n",
        "            # train_loss += loss.item()  # Uncomment after implementing\n",
        "\n",
        "        # =====================================================================\n",
        "        # VALIDATION PHASE\n",
        "        # =====================================================================\n",
        "        model.eval()  # Disable dropout for evaluation\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation (saves memory)\n",
        "            for inputs, labels in val_loader:\n",
        "                # -------------------------------------------------------------\n",
        "                # TODO 10: Implement validation step\n",
        "                # -------------------------------------------------------------\n",
        "                # Only forward pass needed (no backward, no optimizer))\n",
        "                # -------------------------------------------------------------\n",
        "\n",
        "                pass\n",
        "\n",
        "                # val_loss += loss.item()  # Uncomment after implementing\n",
        "\n",
        "        # Calculate average losses\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss   = val_loss   / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopping(avg_val_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRAINING COMPLETED\")\n",
        "    print(f\"Best validation loss: {early_stopping.best_val_loss:.4f}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
        "\n",
        "    return model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d653aafc",
      "metadata": {
        "id": "d653aafc"
      },
      "source": [
        "---\n",
        "## Section 7: Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f941fef4",
      "metadata": {
        "id": "f941fef4"
      },
      "outputs": [],
      "source": [
        "def test_model(model):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model to eva luate\n",
        "\n",
        "    Returns:\n",
        "        test_acc: Test accuracy (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            # -----------------------------------------------------------------\n",
        "            # TODO 11: Implement test prediction\n",
        "            # -----------------------------------------------------------------\n",
        "            # STEP A: Forward pass\n",
        "            #         outputs = model(inputs)\n",
        "            #\n",
        "            # STEP B: Get predicted class (index of maximum logit)\n",
        "            #         preds = outputs.argmax(dim=1)\n",
        "            #         WHY argmax? Each output neuron gives a score for its class.\n",
        "            #         The predicted class is the one with the highest score.\n",
        "            #         Check argmax documentation if needed: https://docs.pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax\n",
        "            # -----------------------------------------------------------------\n",
        "\n",
        "            pass\n",
        "\n",
        "            # all_preds.append(preds)    # Uncomment after implementing\n",
        "            # all_labels.append(labels)  # Uncomment after implementing\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    # Resource: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
        "    test_acc = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
        "\n",
        "    print(f\"TEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf27631",
      "metadata": {
        "id": "2cf27631"
      },
      "source": [
        "---\n",
        "## Section 8: Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4026abde",
      "metadata": {
        "id": "4026abde"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "    \"\"\"\n",
        "    Plot training and validation losses over epochs.\n",
        "\n",
        "    A healthy training curve shows:\n",
        "    - Both losses decreasing initially\n",
        "    - Validation loss eventually plateauing or slightly increasing (overfitting starts)\n",
        "    - Gap between train and val loss indicates overfitting severity\n",
        "    \"\"\"\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO 12: Implement loss plotting\n",
        "    # -------------------------------------------------------------------------\n",
        "    #   # Set figure size\n",
        "    #   # Plot training and validation losses\n",
        "    #   # Put xlabel, ylabel, title, legend, grid\n",
        "    #   # plt.show()\n",
        "    # -------------------------------------------------------------------------\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825c0e36",
      "metadata": {
        "id": "825c0e36"
      },
      "source": [
        "---\n",
        "## Section 9: Run the Experiment\n",
        "\n",
        "# Ablation Study Instructions\n",
        "\n",
        "Here you can run your ablation studies manually. What we recommend is that you change the hyperparameters below and also the number of weights in the model. You might also adjust the early stopping patience parameter. You can have a few objectives:\n",
        "\n",
        "- Get â‰ˆ90%-95% test accuracy score\n",
        "- Try to overfit and observe the validation and training curves\n",
        "- Try to underfit and observe it\n",
        "- Observe how different hyperparameters actually affect the training (for example, you can set the learning rate to 100, set lambda_1 to 1, or set dropout to 0.9 and observe what kind of different scenarios you encounter)\n",
        "\n",
        "In order to do these objectives, you should understand the code and change the correct parameters. If you are able to apply these different settings and accomplish these scenarios, that is a good sign.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c88e8187",
      "metadata": {
        "id": "c88e8187"
      },
      "outputs": [],
      "source": [
        "l1_lambda = 0.001      # L1 regularization strength\n",
        "l2_lambda = 0.01       # L2 regularization strength (instead of implementing this by hand, you can also play with SGD weight_decay parameter)\n",
        "dropout_rate = 0.3     # Dropout probability (try: 0.0, 0.3, 0.5, 0.7)\n",
        "weight_init = 'kaiming'  # Weight initialization ('xavier' or 'kaiming')\n",
        "learning_rate = 0.01   # Learning rate (ou can try 10, 1000, crazy learning rates and fail to learn)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HYPERPARAMETER CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"   L1 Lambda:      {l1_lambda}\")\n",
        "print(f\"   L2 Lambda:      {l2_lambda}\")\n",
        "print(f\"   Dropout Rate:   {dropout_rate}\")\n",
        "print(f\"   Weight Init:    {weight_init}\")\n",
        "print(f\"   Learning Rate:  {learning_rate}\")\n",
        "print(\"=\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe7eeb8",
      "metadata": {
        "id": "dfe7eeb8"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model, train_losses, val_losses = train_model(\n",
        "    l1_lambda=l1_lambda,\n",
        "    l2_lambda=l2_lambda,\n",
        "    dropout_rate=dropout_rate,\n",
        "    weight_init=weight_init,\n",
        "    lr=learning_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf9e3a5",
      "metadata": {
        "id": "ebf9e3a5"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plot_losses(train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50066cb7",
      "metadata": {
        "id": "50066cb7"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393a31d5",
      "metadata": {
        "id": "393a31d5"
      },
      "source": [
        "---\n",
        "## Section 10: Hyperparameter Tuning - Grid Search\n",
        "\n",
        "### CONCEPT: Grid Search\n",
        "\n",
        "Grid Search systematically tries **ALL combinations** of hyperparameters. It is exponentially expensive (3 Ã— 3 Ã— 3 Ã— 2 = 54 combinations). If each training takes 2 minutes, 54 combos = 108 minutes :((("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6db39ac",
      "metadata": {
        "id": "f6db39ac"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter search space for experimentation. You can reduce the options for quicker runs.\n",
        "param_grid = {\n",
        "    'lr': [0.0001, 0.001],       # Learning rates\n",
        "    'dropout_rate': [0.3, 0.5, 0.7],         # Dropout rates\n",
        "    'l1_lambda': [0.0, 0.001],               # L1 regularization\n",
        "    'l2_lambda': [0.0, 0.001],               # L2 regularization\n",
        "    'weight_init': ['xavier', 'kaiming']     # Initialization methods\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292bf139",
      "metadata": {
        "id": "292bf139"
      },
      "outputs": [],
      "source": [
        "def grid_search(param_grid):\n",
        "    \"\"\"\n",
        "    Perform grid search over hyperparameter space.\n",
        "\n",
        "    Args:\n",
        "        param_grid: Dictionary of hyperparameter lists to search\n",
        "\n",
        "    Returns:\n",
        "        best_model: Model with best validation loss\n",
        "        best_params: Dictionary of best hyperparameters\n",
        "    \"\"\"\n",
        "    best_val_loss = float('inf')\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO 13: Generate all hyperparameter combinations\n",
        "    # -------------------------------------------------------------------------\n",
        "    # HINT:\n",
        "    #   keys = list(param_grid.keys())\n",
        "    #   combinations = use itertools.product to get all combos\n",
        "    #\n",
        "    #   Then iterate: for combo in combinations:\n",
        "    #       params = dict(zip(keys, combo))\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    keys = None\n",
        "    combinations = None\n",
        "\n",
        "    print(f\"Grid Search: Testing {len(combinations)} combinations\\n\")\n",
        "\n",
        "    for i, combo in enumerate(combinations):\n",
        "        params = None\n",
        "\n",
        "        print(f\"[{i+1}/{len(combinations)}] Testing: {params}\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # TODO 14: Train model with current params and track best\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        pass\n",
        "\n",
        "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "    return best_model, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67975736",
      "metadata": {
        "id": "67975736"
      },
      "outputs": [],
      "source": [
        "# WARNING: This might take a long time! Only run if you have time to spare.\n",
        "# Uncomment to run grid search:\n",
        "\n",
        "# best_model_grid, best_params_grid = grid_search(param_grid)\n",
        "# print(\"Testing best model from Grid Search:\")\n",
        "# test_model(best_model_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12756073",
      "metadata": {
        "id": "12756073"
      },
      "source": [
        "---\n",
        "## Section 11: Hyperparameter Tuning - Random Search\n",
        "\n",
        "### CONCEPT: Random Search\n",
        "\n",
        "Random Search samples **random combinations** from the hyperparameter space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc6c48b",
      "metadata": {
        "id": "cdc6c48b"
      },
      "outputs": [],
      "source": [
        "def random_search(param_grid, n_iterations=10):\n",
        "    \"\"\"\n",
        "    Perform random search over hyperparameter space.\n",
        "    It will look like grid search but you will sample random combinations.\n",
        "\n",
        "    Args:\n",
        "        param_grid: Dictionary of hyperparameter lists to sample from\n",
        "        n_iterations: Number of random combinations to try\n",
        "\n",
        "    Returns:\n",
        "        best_model: Model with best validation loss\n",
        "        best_params: Dictionary of best hyperparameters\n",
        "    \"\"\"\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO 15: Implement random search\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21b22e3",
      "metadata": {
        "id": "b21b22e3"
      },
      "outputs": [],
      "source": [
        "# best_model_random, best_params_random = random_search(param_grid, n_iterations=10)\n",
        "# print(\"\\nTesting best model from Random Search:\")\n",
        "# test_model(best_model_random)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}